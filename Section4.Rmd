---
title: "Section 4: Importance Sampling and Rejection Sampling"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```

In this section we'll review importance sampling and rejection sampling.

## Rejection Sampling

Rejection sampling is easiest to think of graphically. We first plot the probability density function or probability mass function of the distribution that we want to sample from. If we can uniformly sample from a region that bounds this pdf/pmf, then we can do rejection sampling by:

1. Drawing a uniform random number from the larger region
2. Using the random number if it falls inside the pdf/pmf or throwing it away otherwise

```{r}
N = 1000

alpha = 2.0
beta = 3.0
mode = (alpha - 1) / (alpha + beta - 2)
maxB = dbeta(mode, alpha, beta)

list(x = runif(N),
     y = runif(N) * maxB) %>%
  as.tibble %>%
  mutate(use = y < dbeta(x, alpha, beta)) %>%
  ggplot(aes(x, y)) +
  geom_point(aes(color = use)) +
  stat_function(fun = function(x) dbeta(x, alpha, beta)) +
  geom_rect(aes(xmin = 0.0, xmax = 1.0, ymin = 0.0, ymax = maxB), alpha = 0.0, color = "red") +
  ggtitle("Proposal distribution outlined in red, target in black")
```

The points highlighted in teal above have the desired distribution. We throw away the reddish samples.

## Importance Sampling

In importance sampling, we want to numerically compute the expectation $\int f(x) p(x) dx$. The simplest way to do this with Monte Carlo is with the sum $\frac{1}{N} \sum_i^N f(x_i)$ where $x_i$ is a sequence of random numbers generated from the distribution $p(x)$.

If we cannot easily generate numbers from $p(x)$ (this happens frequently), then we can use importance sampling to compute the expectation. The first step is to multiply the stuff in the integral by a 1:

$$\int f(x) p(x) dx = \int f(x) \frac{g(x)}{g(x)}p(x)dx$$
In this case $g(x)$ is another probability distribution. If we swap the $p(x)$ and the $g(x)$ in the numerator and make a new variable $\hat{f}(x) = f(x) \frac{p(x)}{g(x)}$, we get

$$\int f(x) \frac{p(x)}{g(x)}p(x)dx = \int \hat{f}(x) g(x) dx$$

We can estimate this integral with $\frac{1}{N} \sum_i^N \hat{f}(x_i)$ where $x_i$ is a sequence of random numbers from the distribution $g(x)$. Expanding out this sum gives us the equation we seek:

$$\frac{1}{N} \sum_i^N f(x_i) \frac{p(x_i)}{g(x_i)}$$
With this we can compute an estimate of $\int f(x) p(x) dx$ without having to compute that integral or know how to generate random numbers with the distribution $p(x)$. All we need is another probability density $g(x)$ from which we can draw random numbers and evaluate the PDF.

The choice of $g(x)$ is non-trivial and greatly affects the efficiency of this method.

Instead of normalizing by N, it's usually better to normalize by $\sum_i^N \frac{p(x_i)}{g(x_i)}$. One of the advantages of this is that $p(x)$ does not need to be a normalized distribution (the normalization constant would cancel out). Another is that it generally lowers the variance of the estimate. You should look elsewhere for more info on this as I (Ben) do not really know much about it other than it is important.

$$\frac{1}{\sum_i^N \frac{p(x_i)}{g(x_i)}} \sum_i^N f(x_i) \frac{p(x_i)}{g(x_i)}$$

## Can we sample from the beta(2.0, 2.0) distribution more efficiently?

```{r}
N = 1000

alpha = 2.0
beta = 3.0
mode = (alpha - 1) / (alpha + beta - 2)
maxB = dbeta(mode, alpha, beta)

x = rnorm(N, mode, 0.3)
y = runif(N) * 1.35 * dnorm(x, mode, 0.3)

# Should always check that these things are working
print(paste0("Computed mean: ", mean(x[y < dbeta(x, alpha, beta)]), ", Exact mean: ", alpha / (alpha + beta)))
print(paste0("Computed variance: ", var(x[y < dbeta(x, alpha, beta)]), ", Exact variance: ", alpha * beta / ((alpha + beta)^2 * (alpha + beta + 1))))

list(x = x,
     y = y) %>%
  as.tibble %>%
  mutate(use = y < dbeta(x, alpha, beta)) %>%
  ggplot(aes(x, y)) +
  geom_point(aes(color = use)) +
  stat_function(fun = function(x) dbeta(x, alpha, beta)) +
  stat_function(fun = function(x) 1.35 * dnorm(x, mode, 0.3), color = "blue") +
  geom_rect(aes(xmin = 0.0, xmax = 1.0, ymin = 0.0, ymax = maxB), alpha = 0.0, color = "red") +
  ggtitle("Square proposal distribution outlined in red,\nGaussian proposal in blue,\nTarget in black")
```

```{r}
N = 1000
P = 1000

alpha = 2.0
beta = 3.0
mode = (alpha - 1) / (alpha + beta - 2)
maxB = dbeta(mode, alpha, beta)

yunif = rep(0, P)
ynorm = rep(0, P)
for(p in 1:1000) {
  # Square proposal
  x = runif(N)
  y = runif(N) * maxB
  yunif[p] = sum(y < dbeta(x, alpha, beta)) / N
  
  # Normal proposal
  x = rnorm(N, mode, 0.3)
  y = 1.35 * dnorm(x, mode, 0.3) * runif(N)
  ynorm[p] = sum(y < dbeta(x, alpha, beta)) / N
}

list(yunif = yunif,
     ynorm = ynorm) %>% as.tibble %>%
  gather(which, y) %>%
  ggplot(aes(y)) +
  geom_histogram(aes(fill = which)) +
  xlab("Acceptance probabilities (this percentage of the samples generated are useful)")
```

## Can we rejection sample from a beta(0.9, 0.9)?

It'd be a trick. The distribution is unbounded in y (dbeta(0, 0.9, 0.9) == inf). We'd need something similarly unbounded I guess. It certainly doesn't seem easy to me, but that doesn't mean it's impossible.

## Can compute expectations over the beta(0.9, 0.9) distribution using importance sampling?

Sure thing. What is a possible proposal distribution? For lack of anything else, let's use a uniform distribution on [0, 1], so $q(x) = 1$. How many samples should we take? It's unclear. Let's just repeat the estimate for a bunch of different numbers of samples and see how it converges.

```{r}
alpha = 0.9
beta = alpha

q = function(x) 1
p = function(x) dbeta(x, alpha, beta)

N = 1000
Ns = 1:N
estimate = rep(0, N)
for(n in Ns) {
  x = runif(n)
  w = p(x) / q(x)
  estimate[n] = sum(x * w) / sum(w)
}

list(Ns = Ns,
     estimate = estimate) %>%
  as.tibble %>%
  ggplot(aes(Ns, estimate)) +
  geom_point() +
  geom_hline(aes(yintercept = alpha / (alpha + beta)), color = "red") +
  ggtitle("Estimates for E[x] varying numbers of samples in black\nTruth in red")

```

## Can we compute expectations over the beta(2.0, 3.0) distribution using importance sampling?

Why not? Let's use the truncated normal distribution from above as a proposal distribution. Let's also compute the variance.

```{r}
alpha = 2.0
beta = 3.0

mode = (alpha - 1) / (alpha + beta - 2)

q = function(x) dnorm(x, mode, 0.3)
p = function(x) dbeta(x, alpha, beta)

N = 1000
Ns = 1:N
estimate = rep(0, N)
for(n in Ns) {
  x = rnorm(n, mode, 0.30)
  w = p(x) / q(x)
  mu = sum(x  * w) / sum(w)
  estimate[n] = sum((x - mu)^2 * w) / sum(w)
}

list(Ns = Ns,
     estimate = estimate) %>%
  as.tibble %>%
  ggplot(aes(Ns, estimate)) +
  geom_point() +
  geom_hline(aes(yintercept = alpha * beta / ((alpha + beta)^2 * (alpha + beta + 1))), color = "red") +
  ggtitle("Estimate of variance for varying numbers of samples in black\nTrue value in red")

```
## How do the parameters of the proposal distribution affect the importance sampling efficiency?

Do you think we did a good job?

Let's compute the empirical variance of an estimator of the variance for a variety of different proposal variances to see what happens.

```{r}
R = 100
sigmas = seq(0.1, 2.5, length = 50)

alpha = 2.0
beta = alpha

q = function(x, sigma) dnorm(x, 0.5, sigma)
p = function(x) dbeta(x, alpha, beta)

N = 1000
tibbles = list()
for(i in 1:length(sigmas)) {
  sigma = sigmas[i]
  estimates = rep(0, R)
  for(r in 1:R) {
    x = rnorm(N, 0.5, sigma)
    w = p(x) / q(x, sigma)
    estimates[r] = sum(sin(pi * x) * w) / sum(w)
  }
  tibbles[[i]] = list(esimates = estimates, sigma = sigma) %>% as.tibble
}

tibbles %>% bind_rows %>%
  ggplot(aes(sigma, esimates)) +
  geom_point(alpha = 0.5)
```

Something between about 0.2 and 0.5 seems to be the most efficient for computing this estimate. Do you think the ideal importance sampling distribution might change if we were computing a different expectation (try it with the code above)?

Another way of diagnosing the efficiency of a proposal distribution is by looking at the distributions of the importance weights. Presumably if the importance weights are closer to one, the estimator is better.

```{r}
R = 100
sigmas = seq(0.1, 2.5, length = 50)

alpha = 2.0
beta = alpha

q = function(x, sigma) dnorm(x, 0.5, sigma)
p = function(x) dbeta(x, alpha, beta)

N = 100
tibbles = list()
for(i in 1:length(sigmas)) {
  sigma = sigmas[i]
  x = rnorm(N, 0.5, sigma)
  ratios = p(x) / q(x, sigma)
  tibbles[[i]] = list(ratios = ratios, sigma = sigma) %>% as.tibble
}

tibbles %>% bind_rows %>%
  ggplot(aes(sigma, log(ratios))) +
  geom_point(alpha = 0.5)

# tibbles %>% bind_rows %>%
#   group_by(sigma) %>%
#   summarize(y = var(log(ratios + 1e-12))) %>%
#   ggplot(aes(sigma, y)) +
#   geom_point(alpha = 0.5)
```

It looks like the lowest variance ratios probably happen around $\sigma = 0.25$. This isn't exactly the same information as above. The distribution of ratios is easier to compute than actually estimating the variances like we did previously.

